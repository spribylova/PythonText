{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TEXT SIMILARITY**"
      ],
      "metadata": {
        "id": "ZU7nbBkwjuMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Jaccard**\n",
        "\n",
        "The Jaccard index (Jaccard Coefficient, Jaccard Dissimilarity, and Jaccard Distance) is a statistic used for gauging the similarity and diversity of sample sets. It is defined in general taking the ratio of two sizes, the intersection size divided by the union size, also called intersection over union.\n",
        "\n",
        "*Similarity 0.143* is close to 0, it indicates that the two sets are quite dissimilar.\n",
        "\n",
        "*Similarity 0.6* is close to 1, it indicates that the two sets are quite similar.\n",
        "\n",
        "\n",
        "Jaccard Similarity = (number of observations in both sets) / (number in either set)"
      ],
      "metadata": {
        "id": "4yCo5zV6sLzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK0hMX89sBxT"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(x,y):\n",
        "  intersection_cardinality = len(set.intersection(*[set(x), set(y)])) # number of observations in both, intersection\n",
        "  union_cardinality = len(set.union(*[set(x), set(y)]))# number of observations in either, union\n",
        "  similarity=intersection_cardinality/float(union_cardinality)\n",
        "  distance=1-similarity\n",
        "  return similarity,distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similar\n",
        "sentences = [\"Digitalisierung wächst trotz Rekordeinnahmen.\",\n",
        "\"Digitalisierung wächst bei Rekordeinnahmen.\"]\n",
        "sentences = [sent.lower().split(\" \") for sent in sentences]\n",
        "J_Similarity,J_Distance = jaccard_similarity(sentences[0], sentences[1])\n",
        "print(\"Jaccard Similarity: \",J_Similarity)\n",
        "print(\"Jaccard Distance: \",J_Distance)"
      ],
      "metadata": {
        "id": "IPapwbYxsSjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dissimilar\n",
        "sentences = [\"Digitalisierung wächst trotz Rekordeinnahmen.\",\n",
        "\"Digitalisierung hat unvorstellbare Folgen.\"]\n",
        "sentences = [sent.lower().split(\" \") for sent in sentences]\n",
        "J_Similarity,J_Distance = jaccard_similarity(sentences[0], sentences[1])\n",
        "print(\"Jaccard Similarity: \",J_Similarity)\n",
        "print(\"Jaccard Distance: \",J_Distance)"
      ],
      "metadata": {
        "id": "vCatBBHMVja0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Euclidean Distance**\n",
        "\n",
        "Euclidian distance or Euclidean Metric represents the length of a line segment between two points, which can be calculated by the Pythagorean Theorem.\n",
        "\n",
        "According to the Euclidian distance, the shorter the distance between the two texts is, the more similar they are. Thus, text 2 is more similar to text 3. Long sentences tend to have higher Euclideum score than the short ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "w5erchG-sNB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "33jHzFgItWur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cAiFYe4JuSyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Sie investieren in Medien und Digitalisierung\"\n",
        "text_2 = \"Digitalisierung hat unvorstellbare Folgen\"\n",
        "text_3 = \"Digitalisierung: Förderprogramme für Unternehmen 2025\"\n",
        "\n",
        "## Create a list of the sentences\n",
        "texts = [text_1, text_2, text_3]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Firstly let's count the words using the CountVectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words=[\"ein\",\"das\",\"der\",\"die\",\"den\"]) # full list of german stop words is online available\n",
        "count_vectorizer = CountVectorizer()\n",
        "matrix = count_vectorizer.fit_transform(texts)\n",
        "\n",
        "## we can create a dataframe to represent the number of the words in every sentence\n",
        "table = matrix.todense()\n",
        "df = pd.DataFrame(table,\n",
        "                  columns=count_vectorizer.get_feature_names_out(),\n",
        "                  index=['text_1', 'text_2', 'text_2'])\n",
        "df"
      ],
      "metadata": {
        "id": "zFW9jFzhuMNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Euclidean distance of these sentences.\n",
        "# Shorter the distance between the two texts is, the more similar they are. Thus, text 2 is more similar to text 3.\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "matrix = distance.cdist(df, df, 'euclidean')\n",
        "\n",
        "df_eucl = pd.DataFrame(matrix,\n",
        "                  columns= [\"Text_1\", \"Text_2\", \"Text_3\"],\n",
        "                  index=['text_1', 'text_2', 'text_3'])\n",
        "df_eucl"
      ],
      "metadata": {
        "id": "1WC7GwQEyaEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Count vectoriser and cosine similarity**\n",
        "\n",
        "Using sklearn cosine_similarity and CountVectoriser. CountVectorizer is generally used for featurization of text data whereas OneHotEncoder is only used for featurization of categorical variables. One-hot vectors are high-dimensional and sparse, while word embeddings are low-dimensional and dense.\n",
        "\n",
        "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
        "\n",
        "To overcome this , we use TfidfVectorizer .\n",
        "\n",
        "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.\n"
      ],
      "metadata": {
        "id": "sWkicajUsNtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines = [\n",
        "#Finanzen\n",
        "'Staatsverschuldung wächst trotz Rekordeinnahmen',\n",
        "'Staatsverschuldung ist mehr als nur eine Sonderzahlung',\n",
        "\n",
        "#Digitalisierung\n",
        "'Digitalisierung: Förderprogramme für Unternehmen 2025',\n",
        "'Sie investieren in Medien und Digitalisierung',\n",
        "'Digitalisierung wird unvorstellbare Folgen haben',\n",
        "\n",
        "#Kultur\n",
        "'Kunst oder Kommunikation: Wie trennbar ist das Werk vom Künstler?']"
      ],
      "metadata": {
        "id": "TrLHIXx7sFeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [headline[:20] for headline in headlines]\n",
        "\n",
        "def create_heatmap(similarity, cmap = \"Greys\"):\n",
        "  df = pd.DataFrame(similarity)\n",
        "  df.columns = labels\n",
        "  df.index = labels\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "  sns.heatmap(df, cmap=cmap,annot=True)"
      ],
      "metadata": {
        "id": "lqfvx-JI0g1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "2_-0mUTD0rGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(headlines)\n",
        "arr = X.toarray()\n",
        "\n",
        "create_heatmap(cosine_similarity(arr))"
      ],
      "metadata": {
        "id": "BS9MbvuH0g53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "\n",
        "statistical measure used in information retrieval and machine learning to evaluate the importance of a word in a document relative to a collection of documents.\n",
        "\n",
        "TF IDF - close to 0 = not informative\n",
        "\n",
        "TF IDF - close to 1 = very similar, informative"
      ],
      "metadata": {
        "id": "4yuUqiYJsOfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(headlines)\n",
        "arr = X.toarray()\n",
        "\n",
        "create_heatmap(cosine_similarity(arr))"
      ],
      "metadata": {
        "id": "fSx7wpgJsFht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Word2Vec**\n",
        "\n",
        "Word2vec is a open source tool to calculate the words distance provided by Google. It can be used by inputting a word and output the ranked word lists according to the similarity"
      ],
      "metadata": {
        "id": "SIk9tUHTsPU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download de_core_news_md\n",
        "!python -m spacy download de_core_news_lg"
      ],
      "metadata": {
        "id": "U_aIIEt-h61b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "docs = [nlp(headline) for headline in headlines]"
      ],
      "metadata": {
        "id": "X8Zw3bLP1vxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = []\n",
        "for i in range(len(docs)):\n",
        "      row = []\n",
        "      for j in range(len(docs)):\n",
        "          row.append(docs[i].similarity(docs[j]))\n",
        "      similarity.append(row)\n",
        "create_heatmap(similarity)"
      ],
      "metadata": {
        "id": "9_V2mwz5hZbj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].vector)"
      ],
      "metadata": {
        "id": "gQDuOJS-hZgi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = []\n",
        "for i in range(len(docs)):\n",
        "    row = []\n",
        "    for j in range(len(docs)):\n",
        "      row.append(docs[i].similarity(docs[j]))\n",
        "    similarity.append(row)\n",
        "create_heatmap(similarity)"
      ],
      "metadata": {
        "id": "-2enFUY5hZlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLX4dV6OhZr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.1. Cosine Similarity Torch**"
      ],
      "metadata": {
        "id": "ftTVOYWosQHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)"
      ],
      "metadata": {
        "id": "97BFwkP5kZ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "spM04GqlkZ5i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "TdGEEDGkkjr-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "9orOnSwVkj6O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "\n",
        "torch_geometric.__version__"
      ],
      "metadata": {
        "id": "FBoiztpGkaDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(rc={'axes.facecolor':'dimgrey', 'grid.color':'lightgrey'})\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch_scatter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_undirected"
      ],
      "metadata": {
        "id": "0PxCntffkaI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "m6uctxzYkvXJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"Digitalisierung: Förderprogramme für Unternehmen 2025.\"\n",
        "sentence2 = \"Sie investieren in Medien und Digitalisierung.\"\n",
        "\n",
        "# encode sentences to get their embeddings\n",
        "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "# cosinus similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "print(\"Sentence 1:\", sentence1)\n",
        "print(\"Sentence 2:\", sentence2)\n",
        "print(\"Similarity score:\", cosine_scores.item())"
      ],
      "metadata": {
        "id": "yxIzsraokvcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences1 = [\"Sie investieren in Medien und Digitalisierung.\", \"Digitalisierung wird unvorstellbare Folgen haben.\", \"Digitalisierung: Förderprogramme für Unternehmen 2025.\"]\n",
        "sentences2 = [\"Staatsverschuldung ist mehr als nur eine Sonderzahlung.\", \"Staatsverschuldung wächst trotz Rekordeinnahmen.\"]\n",
        "\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "for i in range(len(sentences1)):\n",
        "    for j in range(len(sentences2)):\n",
        "        print(\"Sentence 1:\", sentences1[i])\n",
        "        print(\"Sentence 2:\", sentences2[j])\n",
        "        print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "        print()"
      ],
      "metadata": {
        "id": "1zKGrQXTkvh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2. Cosine Similarity Vectorizer**\n",
        "\n",
        "\n",
        "Cosine Similarity computes the similarity of two vectors as the cosine of the angle between two vectors. It determines whether two vectors are pointing in roughly the same direction. So if the angle between the vectors is 0 degrees, then the cosine similarity is 1.\n",
        "\n",
        "\n",
        "The Cosine of an angle can take a value between -1 and 1. Speaking from the NLP perspective, this value could be between 0 and 1. If a word does not appear in one of the texts, the fraction becomes zero."
      ],
      "metadata": {
        "id": "28dz7rn0cxiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Sie investieren in Medien und Digitalisierung\"\n",
        "text_2 = \"Digitalisierung hat unvorstellbare Folgen\"\n",
        "text_3 = \"Digitalisierung: Förderprogramme für Unternehmen 2025\"\n",
        "\n",
        "texts = [text_1, text_2, text_3]\n",
        "print(texts)"
      ],
      "metadata": {
        "id": "IucrPA2Sc58f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Constract again the bag of words table\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "YR6s6PdDc8_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words=[\"ein\",\"das\",\"der\",\"die\",\"den\"])\n",
        "count_vectorizer = CountVectorizer()\n",
        "matrix = count_vectorizer.fit_transform(texts)"
      ],
      "metadata": {
        "id": "2Og_ks8ddOLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a data frame to represent the number of the words in every sentence\n",
        "table = matrix.todense()\n",
        "df = pd.DataFrame(table,\n",
        "                  columns=count_vectorizer.get_feature_names_out(),\n",
        "                  index=['text_1', 'text_2', 'text_2'])"
      ],
      "metadata": {
        "id": "0q2LJtiUdORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Aplying the Cosine similarity module, scale is 0 - 1, closer to 1 means more similar\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "values = cosine_similarity(df, df)\n",
        "df = pd.DataFrame(values, columns=[\"Text 1\", \"Text 2\", \"Text 3\"], index = [\"Text 1\", \"Text 2\", \"Text 3\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "UKNAJpmYdOYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJrlr51Uc9LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. ElMo Embeddings from Language Models**\n",
        "\n",
        "There is repository of models: https://vectors.nlpl.eu/repository/\n",
        "You can download Elmo model for german wikipedia language here: 201,German Wikipedia Dump of March 2020. The model has 200 MB and has a rich contextual information.\n",
        "\n",
        "You can use model online from Kaggle pages:"
      ],
      "metadata": {
        "id": "paxxpbCoi2Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simple_elmo"
      ],
      "metadata": {
        "id": "twT0BS-sR2NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download de_core_news_md\n",
        "!python -m spacy download de_core_news_lg"
      ],
      "metadata": {
        "id": "5LGMkE9oUNjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "docs = [nlp(headline) for headline in headlines]"
      ],
      "metadata": {
        "id": "zs1MJ4UXJ5ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf # compatible only for multilingual USE, large USE, EN-DE USE, ...\n",
        "#import tensorflow as tf # version 2 is compatible only for USE 4, ..\n",
        "import tensorflow_hub as hub\n",
        "import spacy\n",
        "import logging\n",
        "from scipy import spatial\n",
        "from simple_elmo import ElmoModel"
      ],
      "metadata": {
        "id": "v7h3_-2aLaH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow\n",
        "\n",
        "# elmo = hub.Module('path if downloaded/Elmo_dowmloaded', trainable=False)\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\") # 100% optimised only for english language\n",
        "\n",
        "tensor_of_strings = tf.constant([\"Grau\",\"Schnell\",\"Langsam\"])\n",
        "elmo.signatures['default'](tensor_of_strings)"
      ],
      "metadata": {
        "id": "Z2kBSMECVz9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "RRC9xtSRN0jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ElmoModel()\n",
        "de_model=\"201.zip\" # locate the downloaded zip file into compiler\n",
        "model.load(de_model)\n",
        "\n",
        "sentence = \"Wikipedia ist ein Projekt zum Aufbau einer Enzyklopädie aus freien Inhalten, zu denen du sehr gern beitragen kannst.\""
      ],
      "metadata": {
        "id": "L3gJmwviTlYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo_vectors = model.get_elmo_vectors(sentence, layers=\"average\")\n",
        "print(f\"Tensor shape: {elmo_vectors.shape}\")"
      ],
      "metadata": {
        "id": "ddSLNs_9TtdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Projekt = np.sum(elmo_vectors[0][29:33], axis = 0)/4\n",
        "Aufbau = np.sum(elmo_vectors[0][45:49], axis = 0)/4\n",
        "Inhalten = np.sum(elmo_vectors[0][87:91], axis = 0)/4"
      ],
      "metadata": {
        "id": "eePS4FFQT0Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Projekt = Projekt.reshape(1,-1)\n",
        "Aufbau = Aufbau.reshape(1,-1)\n",
        "Inhalten = Inhalten.reshape(1,-1)"
      ],
      "metadata": {
        "id": "kGX9v_XASnwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff_1 = cosine_similarity(Projekt, Aufbau)\n",
        "diff_2 = cosine_similarity(Aufbau, Inhalten)\n",
        "same = cosine_similarity(Projekt, Inhalten)\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_1)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_2)"
      ],
      "metadata": {
        "id": "tkkDKsMpTuCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Roberta**\n",
        "\n",
        "The all-roberta-large-v1 model is a sentence transformer developed by the sentence-transformers team. It maps sentences and paragraphs to a 1024-dimensional dense vector space, enabling tasks like clustering and semantic search. This model is based on the RoBERTa architecture and can be used through the sentence-transformers library or directly with the HuggingFace Transformers library."
      ],
      "metadata": {
        "id": "pmoncHX0i2iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
      ],
      "metadata": {
        "id": "AE66b9EiV1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "#Finanzen\n",
        "\"Staatsverschuldung wächst trotz Rekordeinnahmen\",\n",
        "\"Staatsverschuldung ist mehr als nur eine Sonderzahlung\"]"
      ],
      "metadata": {
        "id": "vdSJFDYdVZ8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)\n",
        "\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)\n",
        "# With a high similarity score of 0.7860 this model is accurate and sentences are very similar."
      ],
      "metadata": {
        "id": "GDlx4VjCUaYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Universal Sentence Encoder (USE by Google)**\n",
        "\n",
        "\n",
        "The universal sentence encoder model encodes textual data into high dimensional vectors known as embeddings which are numerical representations of the textual data. It specifically targets transfer learning to other NLP tasks, such as text classification, semantic similarity, and clustering. The pre-trained Universal Sentence Encoder is publicly available in Tensorflow-hub.\n",
        "\n",
        "It is trained on a variety of data sources to learn for a wide variety of tasks. The sources are Wikipedia, web news, web question-answer pages, and discussion forums.\n",
        "\n",
        "**XLING** model is trained for english and german and is compatible with tensorflow version 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "qDjSkfRDy68u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf_sentencepiece"
      ],
      "metadata": {
        "id": "t7ntW4XmC3OX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "EWCtTlW6DcpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.18.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TKXiJzhUam2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf # compatible only for multilingual USE, large USE, EN-DE USE, ...\n",
        "#import tensorflow as tf # version 2 is compatible only for USE 4, ..\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "D5oT37r6sFo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "#model_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/1\"\n",
        "model_url = \"https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1\"\n",
        "#model = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)\n",
        "\n",
        "df = pd.DataFrame(columns=[\"ID\",\"DESCRIPTION\"], data=np.matrix([[10,\"Staatsverschuldung wächst trotz Rekordeinnahmen\"],\n",
        "                                                                [11,\"Staatsverschuldung ist mehr als nur eine Sonderzahlung\"],\n",
        "                                                                [12,\"Digitalisierung: Förderprogramme für Unternehmen 2025\"],\n",
        "                                                                [13,\"Sie investieren in Medien und Digitalisierung\"],\n",
        "                                                                [14,\"Digitalisierung wird unvorstellbare Folgen haben\"],\n",
        "                                                                [15,\"Kunst oder Kommunikation: Wie trennbar ist das Werk vom Künstler\"]\n",
        "                                                                ]))"
      ],
      "metadata": {
        "id": "KQEzzbMPzCyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_embeddings = embed(list(df['DESCRIPTION']))\n",
        "cos_sim = sklearn.metrics.pairwise.cosine_similarity(message_embeddings)"
      ],
      "metadata": {
        "id": "sUa5Oy4nzC41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_similarity(labels, corr_matrix):\n",
        "  sns.set(font_scale=0.9)\n",
        "  g = sns.heatmap(\n",
        "      corr_matrix,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"Greys\",\n",
        "      annot=True)\n",
        "  g.set_xticklabels(labels, rotation=90)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "plot_similarity(list(df['DESCRIPTION']), cos_sim)"
      ],
      "metadata": {
        "id": "P_m_XvbQzC_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. One Hot Encoding**\n",
        "\n",
        "Used for text representation, but decision trees and dictionaries are more evolved."
      ],
      "metadata": {
        "id": "hniv-lcFVTc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "S4pwiAOydOJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "df = pd.DataFrame({\n",
        "    'Farbe': ['rot', 'grün', 'blau']\n",
        "})\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "encoded_df = pd.get_dummies(df['Farbe'])\n",
        "print(encoded_df)"
      ],
      "metadata": {
        "id": "SEzMZmFpVrjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Bag of Words**\n",
        "\n",
        "Bag of Words is useful in many NLP tasks:\n",
        "Feature extraction, Simplicity and efficiency, Document similarity, ...\n",
        "\n",
        "We can use it to calculate the cosine similarity."
      ],
      "metadata": {
        "id": "dnB9l7yFVoAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "pRiPUW_uVsQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A corpus containing a collection of sentences\n",
        "corpus = [\n",
        "\"Staatsverschuldung wächst trotz Rekordeinnahmen\",\n",
        "\"Staatsverschuldung ist mehr als nur eine Sonderzahlung\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ve37ztQRkvn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize vectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "5spkjr3IkvtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit vectorizer to corpus\n",
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "sZQPjQTpkvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View vocabulary\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "kqTXVhxDVsVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow)"
      ],
      "metadata": {
        "id": "FuxTGKXhFz5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense matrix representation\n",
        "bow.toarray()"
      ],
      "metadata": {
        "id": "-EBe2FXOlBDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load english language model\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "\n",
        "# Define custom tokenizer (remove stop words and punctuation and apply lemmatization)\n",
        "def custom_tokenizer(doc):\n",
        "    return [t.lemma_ for t in nlp(doc) if (not t.is_punct) and (not t.is_stop)]"
      ],
      "metadata": {
        "id": "LDjovOZVlBIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass tokenizer as callback function to countvectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, binary=True)\n",
        "\n",
        "# Fit vectorizer to corpus\n",
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "n-K_O1SIllm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "qjHu0JGTllyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense matrix representation\n",
        "bow.toarray()"
      ],
      "metadata": {
        "id": "GGmYU15clBNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse slice\n",
        "print(bow[:,0:4])"
      ],
      "metadata": {
        "id": "b0gYOTJ6lvTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity using numpy\n",
        "def cosine_sim(a,b):\n",
        "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "VokUYLdQlvYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity between two documents\n",
        "print(corpus[0])\n",
        "print(corpus[1])\n",
        "print(f'Similarity score: {cosine_sim(bow[0].toarray().squeeze(),bow[1].toarray().squeeze()):.1f}')"
      ],
      "metadata": {
        "id": "4TFWl3nFl2I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarity(bow))"
      ],
      "metadata": {
        "id": "Bwl2ojsZl2PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpPopxf8l2Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7XmnsKqlv5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}